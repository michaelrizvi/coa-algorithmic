{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Hop Token Usage Analysis\n",
    "\n",
    "This notebook analyzes the average completion tokens as a function of the number of hops for the k-hop reasoning task across different models (exaone, llama8b, llama70b) and fact counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Set LaTeX font settings for publication quality with Times New Roman 12pt\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': True,\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times'],  # Use Times New Roman\n",
    "    'text.latex.preamble': r'\\usepackage{times}',  # LaTeX Times package\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 12,  # 12pt for titles\n",
    "    'axes.labelsize': 12,  # 12pt for axes labels\n",
    "    'legend.fontsize': 10,  # Keep legend at 10pt\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9\n",
    "})\n",
    "\n",
    "print(\"Notebook setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the token usage data\n",
    "token_df = pd.read_csv('data/khop_avg_completion_tokens.csv')\n",
    "\n",
    "print(\"Token usage data loaded successfully.\")\n",
    "print(f\"DataFrame shape: {token_df.shape}\")\n",
    "print(f\"\\nColumns (first 5):\")\n",
    "print([col for col in token_df.columns][:5])\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(token_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_and_facts_info(col_name):\n",
    "    \"\"\"Extract model type and fact count information from column name\"\"\"\n",
    "    \n",
    "    # Extract model type\n",
    "    if 'llama8B' in col_name:\n",
    "        model_type = 'llama8b'\n",
    "    elif 'llama70B' in col_name:\n",
    "        model_type = 'llama70b'\n",
    "    elif 'exaone' in col_name:\n",
    "        model_type = 'exaone'\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "    # Extract number of facts\n",
    "    facts_match = re.search(r'facts(\\d+)', col_name)\n",
    "    num_facts = int(facts_match.group(1)) if facts_match else None\n",
    "    \n",
    "    return model_type, num_facts\n",
    "\n",
    "def organize_token_data(df):\n",
    "    \"\"\"Organize token data by model and fact count\"\"\"\n",
    "    \n",
    "    # Filter columns to get only the main avg_completion_tokens columns (not MIN/MAX)\n",
    "    token_cols = [col for col in df.columns if ' - avg_completion_tokens' in col and 'MIN' not in col and 'MAX' not in col]\n",
    "    \n",
    "    organized_data = {}\n",
    "    \n",
    "    for col in token_cols:\n",
    "        model_type, num_facts = extract_model_and_facts_info(col)\n",
    "        if model_type and num_facts:\n",
    "            key = f\"{model_type}_{num_facts}facts\"\n",
    "            \n",
    "            if key not in organized_data:\n",
    "                organized_data[key] = {\n",
    "                    'model_type': model_type,\n",
    "                    'num_facts': num_facts,\n",
    "                    'hops': [],\n",
    "                    'tokens': []\n",
    "                }\n",
    "            \n",
    "            # Get data for this column\n",
    "            for i, hop in enumerate(df['num_hops']):\n",
    "                token_val = df.iloc[i][col]\n",
    "                if pd.notna(token_val) and token_val != '':\n",
    "                    try:\n",
    "                        organized_data[key]['hops'].append(hop)\n",
    "                        organized_data[key]['tokens'].append(float(token_val))\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "    \n",
    "    return organized_data\n",
    "\n",
    "# Organize the data\n",
    "organized_data = organize_token_data(token_df)\n",
    "\n",
    "print(\"Found the following model/fact combinations:\")\n",
    "for key, data in organized_data.items():\n",
    "    print(f\"  {key}: {len(data['hops'])} data points (hops {min(data['hops'])}-{max(data['hops'])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots showing avg_completion_tokens vs num_hops for each model type\n",
    "# Save separate PDF for each model\n",
    "\n",
    "# Define the three models and colors similar to existing notebooks\n",
    "model_types = ['exaone', 'llama8b', 'llama70b']\n",
    "\n",
    "# Define consistent colors for different fact counts across all plots\n",
    "fact_colors = {\n",
    "    100: '#2E5EAA',   # Red\n",
    "    200: '#2E8B57',   # Blue  \n",
    "    500: '#B22222'    # Green\n",
    "}\n",
    "\n",
    "# Create individual plots for each model type\n",
    "for model_type in model_types:\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    \n",
    "    # Find all fact counts for this model\n",
    "    model_data = {key: data for key, data in organized_data.items() if data['model_type'] == model_type}\n",
    "    \n",
    "    # Sort by fact count for consistent legend ordering\n",
    "    sorted_keys = sorted(model_data.keys(), key=lambda x: model_data[x]['num_facts'])\n",
    "    \n",
    "    for key in sorted_keys:\n",
    "        data = model_data[key]\n",
    "        \n",
    "        # Plot line with markers - X axis is num_hops, Y axis is avg_completion_tokens\n",
    "        # Use consistent colors based on fact count\n",
    "        color = fact_colors.get(data['num_facts'], '#000000')  # Default to black if fact count not found\n",
    "        \n",
    "        ax.plot(\n",
    "            data['hops'],\n",
    "            data['tokens'],\n",
    "            label=f\"{data['num_facts']} facts\",\n",
    "            color=color,\n",
    "            marker='o',\n",
    "            markersize=5,\n",
    "            linewidth=2,\n",
    "            alpha=0.9\n",
    "        )\n",
    "    \n",
    "    # Customize plot with correct axis labels\n",
    "    ax.set_xlabel(r'\\textbf{Number of Hops}')\n",
    "    ax.set_ylabel(r'\\textbf{Computation Depth}')\n",
    "    \n",
    "    # Capitalize model names for titles\n",
    "    model_title = model_type.replace('llama8b', 'Llama-8B').replace('llama70b', 'Llama-70B').replace('exaone', 'ExaOne')\n",
    "    ax.set_title(r'\\textbf{' + f'Token Usage Analysis: {model_title}' + '}')\n",
    "    \n",
    "    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    # Set axis limits\n",
    "    if model_data:\n",
    "        all_tokens = [token for data in model_data.values() for token in data['tokens']]\n",
    "        all_hops = [hop for data in model_data.values() for hop in data['hops']]\n",
    "        \n",
    "        if all_tokens and all_hops:\n",
    "            ax.set_ylim(0, max(all_tokens) * 1.1)\n",
    "            ax.set_xlim(min(all_hops) - 0.5, max(all_hops) + 0.5)\n",
    "            ax.set_xticks(range(min(all_hops), max(all_hops) + 1, 2))  # Show every other hop\n",
    "    \n",
    "    # Add boxed legend with similar styling to other notebooks\n",
    "    if len(model_data) > 0:\n",
    "        legend = ax.legend(frameon=True, loc='upper left', fontsize=10,\n",
    "                          fancybox=True, shadow=True, framealpha=0.95,\n",
    "                          edgecolor='black', facecolor='white')\n",
    "        legend.get_frame().set_linewidth(0.8)\n",
    "    \n",
    "    # Clean up spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save individual plot as PDF\n",
    "    filename = f\"khop_token_usage_{model_type}.pdf\"\n",
    "    plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved plot: {filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also create a combined plot showing all three models together for comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot all models on the same plot, showing all fact counts\n",
    "model_markers = {\n",
    "    'exaone': 'o',     # Circle\n",
    "    'llama8b': 's',    # Square\n",
    "    'llama70b': '^'    # Triangle\n",
    "}\n",
    "\n",
    "plotted_combinations = []\n",
    "\n",
    "for model_type in model_types:\n",
    "    model_data = {key: data for key, data in organized_data.items() if data['model_type'] == model_type}\n",
    "    \n",
    "    for key in sorted(model_data.keys(), key=lambda x: model_data[x]['num_facts']):\n",
    "        data = model_data[key]\n",
    "        \n",
    "        # Use consistent colors for fact counts and different markers for models\n",
    "        color = fact_colors.get(data['num_facts'], '#000000')\n",
    "        marker = model_markers.get(model_type, 'o')\n",
    "        \n",
    "        model_label = model_type.replace('llama8b', 'Llama-8B').replace('llama70b', 'Llama-70B').replace('exaone', 'ExaOne')\n",
    "        label = f\"{model_label} ({data['num_facts']} facts)\"\n",
    "        \n",
    "        ax.plot(\n",
    "            data['hops'],\n",
    "            data['tokens'],\n",
    "            label=label,\n",
    "            color=color,\n",
    "            marker=marker,\n",
    "            markersize=5,\n",
    "            linewidth=2,\n",
    "            alpha=0.8\n",
    "        )\n",
    "        plotted_combinations.append((model_type, data['num_facts']))\n",
    "\n",
    "# Customize plot with correct axis labels\n",
    "ax.set_xlabel(r'\\textbf{Number of Hops}')\n",
    "ax.set_ylabel(r'\\textbf{Computation Depth}')\n",
    "ax.set_title(r'\\textbf{Token Usage Comparison: All Models and Fact Counts}')\n",
    "\n",
    "ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Add boxed legend - use two columns to fit all combinations\n",
    "if plotted_combinations:\n",
    "    legend = ax.legend(frameon=True, loc='upper left', fontsize=9, ncol=2,\n",
    "                      fancybox=True, shadow=True, framealpha=0.95,\n",
    "                      edgecolor='black', facecolor='white')\n",
    "    legend.get_frame().set_linewidth(0.8)\n",
    "\n",
    "# Clean up spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"khop_token_usage_comparison_all_models.pdf\", bbox_inches='tight', dpi=300)\n",
    "print(\"Saved combined comparison plot: khop_token_usage_comparison_all_models.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_and_facts_info(col_name):\n",
    "    \"\"\"Extract model type and fact count information from column name\"\"\"\n",
    "    \n",
    "    # Extract model type\n",
    "    if 'llama8B' in col_name:\n",
    "        model_type = 'llama8b'\n",
    "    elif 'llama70B' in col_name:\n",
    "        model_type = 'llama70b'\n",
    "    elif 'exaone' in col_name:\n",
    "        model_type = 'exaone'\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "    # Extract number of facts\n",
    "    facts_match = re.search(r'facts(\\d+)', col_name)\n",
    "    num_facts = int(facts_match.group(1)) if facts_match else None\n",
    "    \n",
    "    return model_type, num_facts\n",
    "\n",
    "def organize_token_data(df):\n",
    "    \"\"\"Organize token data by model and fact count\"\"\"\n",
    "    \n",
    "    # Filter columns to get only the main step columns (not MIN/MAX)\n",
    "    step_cols = [col for col in df.columns if ' - _step' in col and 'MIN' not in col and 'MAX' not in col]\n",
    "    \n",
    "    organized_data = {}\n",
    "    \n",
    "    for col in step_cols:\n",
    "        model_type, num_facts = extract_model_and_facts_info(col)\n",
    "        if model_type and num_facts:\n",
    "            key = f\"{model_type}_{num_facts}facts\"\n",
    "            \n",
    "            if key not in organized_data:\n",
    "                organized_data[key] = {\n",
    "                    'model_type': model_type,\n",
    "                    'num_facts': num_facts,\n",
    "                    'hops': [],\n",
    "                    'tokens': []\n",
    "                }\n",
    "            \n",
    "            # Get data for this column\n",
    "            for i, hop in enumerate(df['num_hops']):\n",
    "                token_val = df.iloc[i][col]\n",
    "                if pd.notna(token_val) and token_val != '':\n",
    "                    try:\n",
    "                        organized_data[key]['hops'].append(hop)\n",
    "                        organized_data[key]['tokens'].append(float(token_val))\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "    \n",
    "    return organized_data\n",
    "\n",
    "# Organize the data\n",
    "organized_data = organize_token_data(token_df)\n",
    "\n",
    "print(\"Found the following model/fact combinations:\")\n",
    "for key, data in organized_data.items():\n",
    "    print(f\"  {key}: {len(data['hops'])} data points (hops {min(data['hops'])}-{max(data['hops'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "From the token usage analysis, we can observe:\n",
    "\n",
    "1. **Linear Growth**: Computation depth (measured in tokens) appears to grow linearly with the number of hops, which is expected as each hop requires additional reasoning steps.\n",
    "\n",
    "2. **Model Differences**: Different models show varying computational depth patterns, which may reflect differences in their reasoning strategies and efficiency.\n",
    "\n",
    "3. **Fact Count Impact**: The number of facts in the knowledge base affects computational depth, with more facts potentially requiring different amounts of computation.\n",
    "\n",
    "4. **Scaling Behavior**: The consistent linear relationship suggests that the computational complexity scales predictably with problem complexity.\n",
    "\n",
    "This analysis provides insights into the computational efficiency of different models on the k-hop reasoning task and can inform decisions about model selection and resource allocation for complex reasoning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
