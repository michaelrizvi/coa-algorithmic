{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parity Accuracy Analysis: Llama-8B vs Llama-70B\n",
    "\n",
    "This notebook analyzes the parity accuracy results for Llama-8B and Llama-70B models using bar plots with error bars, following the same methodology as the parity-notebook.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure plotting style for ICLR paper format with 12pt font\n",
    "plt.style.use(['science', 'ieee'])  # Enables LaTeX + clean scientific styling\n",
    "\n",
    "# LaTeX font settings - 12pt for ICLR format\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': True,\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 13,\n",
    "    'axes.labelsize': 12,\n",
    "    'legend.fontsize': 11,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file containing llama-8B and llama-70B parity accuracy data\n",
    "df = pd.read_csv('parity_accuracy_llama70B.csv')\n",
    "\n",
    "# Display basic information about the data\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns - remove MIN, MAX, and step columns\n",
    "filtered_df = df[[col for col in df.columns if all(x not in col for x in ['MIN', 'MAX', '_step'])]]\n",
    "print(\"Filtered columns:\")\n",
    "print(filtered_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyperparams(col_name):\n",
    "    \"\"\"Extract hyperparameters from column name\"\"\"\n",
    "    if 'maj-voting' in col_name:\n",
    "        match = re.search(r'agents(\\d+)', col_name)\n",
    "        return int(match.group(1)) if match else None\n",
    "    elif 'coa' in col_name:\n",
    "        match = re.search(r'chunk(\\d+)', col_name)\n",
    "        return int(match.group(1)) if match else None\n",
    "    elif 'prefix-sum' in col_name:\n",
    "        match = re.search(r'b(\\d+)', col_name)\n",
    "        return int(match.group(1)) if match else None\n",
    "    return None\n",
    "\n",
    "def extract_model_info(col_name):\n",
    "    \"\"\"Extract model type (8B or 70B) from column name\"\"\"\n",
    "    if 'llama8B' in col_name:\n",
    "        return '8B'\n",
    "    elif 'llama70B' in col_name:\n",
    "        return '70B'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parity_bar_plot(model_type='8B', title_suffix=''):\n",
    "    \"\"\"\n",
    "    Create bar plot with error bars for parity accuracy.\n",
    "    Enhanced for academic paper presentation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter columns for the specified model type\n",
    "    model_cols = [col for col in filtered_df.columns \n",
    "                  if col != 'sequence_length' and f'llama{model_type}' in col]\n",
    "    \n",
    "    methods = ['prefix-sum', 'maj-voting', 'coa']\n",
    "    sequence_lengths = filtered_df['sequence_length']\n",
    "    best_accs = {method: [] for method in methods}\n",
    "    error_bars = {method: [] for method in methods}\n",
    "    \n",
    "    for method in methods:\n",
    "        method_cols = [col for col in model_cols if method in col and 'avg_accuracy' in col]\n",
    "        \n",
    "        for _, row in filtered_df.iterrows():\n",
    "            # Find best accuracy for this method at this sequence length\n",
    "            method_accs = [row[col] for col in method_cols if pd.notna(row[col])]\n",
    "            \n",
    "            if method_accs:\n",
    "                best_acc = max(method_accs)\n",
    "                best_accs[method].append(best_acc)\n",
    "                \n",
    "                # Calculate standard error: sqrt(p * (1-p) / n) where n=100 runs\n",
    "                std_error = np.sqrt(best_acc * (1 - best_acc) / 100)\n",
    "                error_bars[method].append(std_error)\n",
    "            else:\n",
    "                best_accs[method].append(0)\n",
    "                error_bars[method].append(0)\n",
    "    \n",
    "    # Enhanced plotting for academic papers\n",
    "    bar_width = 0.25\n",
    "    x_pos = range(len(sequence_lengths))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(4.2, 3.4))  # Reduced height back to compact size\n",
    "    # Enhanced colors for better print quality and accessibility\n",
    "    colors = ['#2E5EAA', '#2E8B57', '#B22222']\n",
    "    \n",
    "    bars = []\n",
    "    for i, method in enumerate(methods):\n",
    "        # Change \"coa\" to \"CoA\" in label\n",
    "        if method == 'coa':\n",
    "            label = 'CoA'\n",
    "        else:\n",
    "            label = method.replace('-', ' ').title()\n",
    "            \n",
    "        bar_container = ax.bar(\n",
    "            [p + i * bar_width for p in x_pos],\n",
    "            best_accs[method],\n",
    "            width=bar_width,\n",
    "            label=label,\n",
    "            color=colors[i],\n",
    "            yerr=error_bars[method],\n",
    "            capsize=3,  # Larger caps for visibility\n",
    "            error_kw={'linewidth': 0.8, 'capthick': 0.8, 'ecolor': 'black'},\n",
    "            edgecolor='black',  # Black edges for bars\n",
    "            linewidth=0.5,\n",
    "            alpha=0.9  # Slight transparency for better aesthetics\n",
    "        )\n",
    "        bars.append(bar_container)\n",
    "    \n",
    "    ax.set_xticks([p + bar_width for p in x_pos])\n",
    "    ax.set_xticklabels(sequence_lengths)\n",
    "    ax.set_xlabel(r'\\textbf{Sequence Length}')\n",
    "    ax.set_ylabel(r'\\textbf{Accuracy}')\n",
    "    ax.set_title(r'\\textbf{' + f'Llama-{model_type}: Parity Accuracy' + '}', pad=15)\n",
    "    \n",
    "    # Enhanced legend outside plot at bottom with horizontal orientation and optimized spacing\n",
    "    legend = ax.legend(frameon=True, loc='upper center', bbox_to_anchor=(0.5, -0.2), \n",
    "                      ncol=3, fontsize=8, fancybox=True, shadow=True, framealpha=0.95,\n",
    "                      edgecolor='black', facecolor='white')\n",
    "    legend.get_frame().set_linewidth(0.8)\n",
    "    \n",
    "    # Better grid styling\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=0.6, alpha=0.6, color='gray')\n",
    "    ax.set_axisbelow(True)  # Grid behind bars\n",
    "    \n",
    "    # Clean up spines for professional look\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(0.8)\n",
    "    ax.spines['bottom'].set_linewidth(0.8)\n",
    "    \n",
    "    # Better y-axis formatting\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(nbins=6))\n",
    "    ax.set_ylim(0, 1.05)  # Ensure full accuracy range is visible\n",
    "    \n",
    "    # Optimized layout for compact size\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(bottom=0.25)  # Adjusted for compact figure\n",
    "    \n",
    "    # High-quality output for publication\n",
    "    filename = f\"parity_accuracy_llama{model_type.lower()}.pdf\"\n",
    "    fig.savefig(filename, bbox_inches='tight', dpi=300, facecolor='white')\n",
    "    print(f\"Saved plot as {filename}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return best_accs, error_bars\n",
    "\n",
    "# Create plots for both models\n",
    "print(\"Creating bar plot for Llama-8B...\")\n",
    "#best_accs_8b, error_bars_8b = create_parity_bar_plot('8B')\n",
    "\n",
    "print(\"\\nCreating bar plot for Llama-70B...\")\n",
    "best_accs_70b, error_bars_70b = create_parity_bar_plot('70B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display best hyperparameters for each model\n",
    "def analyze_best_hyperparams(model_type='8B'):\n",
    "    \"\"\"\n",
    "    Extract best hyperparameter values for each agent type and sequence length for specified model\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== LLAMA-{model_type} ANALYSIS ===\")\n",
    "    print(\"Best hyperparameters for each agent type and sequence length:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Filter columns for the specified model type\n",
    "    model_cols = [col for col in filtered_df.columns \n",
    "                  if col != 'sequence_length' and f'llama{model_type}' in col]\n",
    "    \n",
    "    # Create results dictionary\n",
    "    best_hyperparams = {\n",
    "        'maj-voting': {},\n",
    "        'coa': {},  \n",
    "        'prefix-sum': {}\n",
    "    }\n",
    "    \n",
    "    for method in ['maj-voting', 'coa', 'prefix-sum']:\n",
    "        method_cols = [col for col in model_cols if method in col and 'avg_accuracy' in col]\n",
    "        \n",
    "        for seq_len in filtered_df['sequence_length']:\n",
    "            row_data = filtered_df[filtered_df['sequence_length'] == seq_len].iloc[0]\n",
    "            \n",
    "            # Get accuracies for this method and sequence length\n",
    "            method_accs = {}\n",
    "            for col in method_cols:\n",
    "                acc = row_data[col]\n",
    "                if pd.notna(acc):  # Only include non-NaN values\n",
    "                    hyperparam = extract_hyperparams(col)\n",
    "                    if hyperparam is not None:\n",
    "                        method_accs[hyperparam] = acc\n",
    "            \n",
    "            # Find best hyperparameter\n",
    "            if method_accs:\n",
    "                best_hyperparam = max(method_accs.keys(), key=lambda k: method_accs[k])\n",
    "                best_hyperparams[method][seq_len] = {\n",
    "                    'hyperparam': best_hyperparam,\n",
    "                    'accuracy': method_accs[best_hyperparam]\n",
    "                }\n",
    "    \n",
    "    # Display results\n",
    "    for method in ['maj-voting', 'coa', 'prefix-sum']:\n",
    "        if method == 'maj-voting':\n",
    "            method_display = \"MAJ VOTING\"\n",
    "            param_name = \"num_agents\"\n",
    "        elif method == 'coa':\n",
    "            method_display = \"CoA\"\n",
    "            param_name = \"chunk_size\"\n",
    "        else:  # prefix-sum\n",
    "            method_display = \"PREFIX SUM\"\n",
    "            param_name = \"branching_factor\"\n",
    "            \n",
    "        print(f\"\\n{method_display}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for seq_len in sorted(best_hyperparams[method].keys()):\n",
    "            result = best_hyperparams[method][seq_len]\n",
    "            print(f\"Seq length {seq_len:3d}: {param_name}={result['hyperparam']:2d}, accuracy={result['accuracy']:.3f}\")\n",
    "    \n",
    "    return best_hyperparams\n",
    "\n",
    "# Analyze both models\n",
    "best_hyperparams_8b = analyze_best_hyperparams('8B')\n",
    "best_hyperparams_70b = analyze_best_hyperparams('70B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side comparison plot\n",
    "def create_model_comparison_plot():\n",
    "    \"\"\"\n",
    "    Create a side-by-side comparison of both models showing best accuracy for each method.\n",
    "    Enhanced for academic paper presentation.\n",
    "    \"\"\"\n",
    "    \n",
    "    methods = ['prefix-sum', 'maj-voting', 'coa']\n",
    "    sequence_lengths = filtered_df['sequence_length']\n",
    "    \n",
    "    # Collect best accuracies for both models\n",
    "    model_data = {}\n",
    "    \n",
    "    for model_type in ['8B', '70B']:\n",
    "        model_cols = [col for col in filtered_df.columns \n",
    "                      if col != 'sequence_length' and f'llama{model_type}' in col]\n",
    "        \n",
    "        best_accs = {method: [] for method in methods}\n",
    "        \n",
    "        for method in methods:\n",
    "            method_cols = [col for col in model_cols if method in col and 'avg_accuracy' in col]\n",
    "            \n",
    "            for _, row in filtered_df.iterrows():\n",
    "                method_accs = [row[col] for col in method_cols if pd.notna(row[col])]\n",
    "                \n",
    "                if method_accs:\n",
    "                    best_acc = max(method_accs)\n",
    "                    best_accs[method].append(best_acc)\n",
    "                else:\n",
    "                    best_accs[method].append(0)\n",
    "        \n",
    "        model_data[model_type] = best_accs\n",
    "    \n",
    "    # Enhanced comparison plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8.5, 4.5), sharey=True)  # Increased height for legend\n",
    "    colors = ['#2E5EAA', '#2E8B57', '#B22222']  # Enhanced colors\n",
    "    bar_width = 0.25\n",
    "    x_pos = range(len(sequence_lengths))\n",
    "    \n",
    "    # Plot Llama-8B\n",
    "    for i, method in enumerate(methods):\n",
    "        # Change \"coa\" to \"CoA\" in label\n",
    "        if method == 'coa':\n",
    "            label = 'CoA'\n",
    "        else:\n",
    "            label = method.replace('-', ' ').title()\n",
    "            \n",
    "        ax1.bar(\n",
    "            [p + i * bar_width for p in x_pos],\n",
    "            model_data['8B'][method],\n",
    "            width=bar_width,\n",
    "            label=label,\n",
    "            color=colors[i],\n",
    "            edgecolor='black',\n",
    "            linewidth=0.5,\n",
    "            alpha=0.9\n",
    "        )\n",
    "    \n",
    "    ax1.set_xticks([p + bar_width for p in x_pos])\n",
    "    ax1.set_xticklabels(sequence_lengths)\n",
    "    ax1.set_xlabel(r'\\textbf{Sequence Length}')\n",
    "    ax1.set_ylabel(r'\\textbf{Accuracy}')\n",
    "    ax1.set_title(r'\\textbf{Llama-8B}', pad=15)\n",
    "    \n",
    "    ax1.grid(True, axis='y', linestyle='--', linewidth=0.6, alpha=0.6, color='gray')\n",
    "    ax1.set_axisbelow(True)\n",
    "    \n",
    "    # Plot Llama-70B\n",
    "    for i, method in enumerate(methods):\n",
    "        # Change \"coa\" to \"CoA\" in label\n",
    "        if method == 'coa':\n",
    "            label = 'CoA'\n",
    "        else:\n",
    "            label = method.replace('-', ' ').title()\n",
    "            \n",
    "        ax2.bar(\n",
    "            [p + i * bar_width for p in x_pos],\n",
    "            model_data['70B'][method],\n",
    "            width=bar_width,\n",
    "            label=label,\n",
    "            color=colors[i],\n",
    "            edgecolor='black',\n",
    "            linewidth=0.5,\n",
    "            alpha=0.9\n",
    "        )\n",
    "    \n",
    "    ax2.set_xticks([p + bar_width for p in x_pos])\n",
    "    ax2.set_xticklabels(sequence_lengths)\n",
    "    ax2.set_xlabel(r'\\textbf{Sequence Length}')\n",
    "    ax2.set_title(r'\\textbf{Llama-70B}', pad=15)\n",
    "    \n",
    "    ax2.grid(True, axis='y', linestyle='--', linewidth=0.6, alpha=0.6, color='gray')\n",
    "    ax2.set_axisbelow(True)\n",
    "    \n",
    "    # Clean up spines for both plots\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_linewidth(0.8)\n",
    "        ax.spines['bottom'].set_linewidth(0.8)\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(nbins=6))\n",
    "        ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Add shared legend at bottom of figure with more spacing\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.08), \n",
    "              ncol=3, fontsize=11, fancybox=True, shadow=True, framealpha=0.95,\n",
    "              edgecolor='black', facecolor='white')\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(bottom=0.18)  # Add extra space at bottom for legend\n",
    "    \n",
    "    fig.savefig(\"parity_accuracy_model_comparison.pdf\", bbox_inches='tight', dpi=300, facecolor='white')\n",
    "    print(\"Saved comparison plot as parity_accuracy_model_comparison.pdf\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "create_model_comparison_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics and comparison\n",
    "def print_summary_comparison():\n",
    "    \"\"\"\n",
    "    Print summary statistics comparing the two models\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY COMPARISON: LLAMA-8B vs LLAMA-70B\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    methods = ['prefix-sum', 'maj-voting', 'coa']\n",
    "    \n",
    "    for method in methods:\n",
    "        if method == 'prefix-sum':\n",
    "            method_display = \"PREFIX SUM\"\n",
    "        elif method == 'maj-voting':\n",
    "            method_display = \"MAJ VOTING\"\n",
    "        else:  # coa\n",
    "            method_display = \"CoA\"\n",
    "            \n",
    "        print(f\"\\n{method_display} METHOD:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate average accuracy across all sequence lengths\n",
    "        avg_8b = np.mean(best_accs_8b[method])\n",
    "        avg_70b = np.mean(best_accs_70b[method])\n",
    "        \n",
    "        print(f\"Average accuracy - Llama-8B:  {avg_8b:.3f}\")\n",
    "        print(f\"Average accuracy - Llama-70B: {avg_70b:.3f}\")\n",
    "        print(f\"Improvement (70B vs 8B): {avg_70b - avg_8b:+.3f} ({((avg_70b/avg_8b - 1) * 100):+.1f}%)\")\n",
    "    \n",
    "    # Overall comparison\n",
    "    overall_8b = np.mean([np.mean(best_accs_8b[method]) for method in methods])\n",
    "    overall_70b = np.mean([np.mean(best_accs_70b[method]) for method in methods])\n",
    "    \n",
    "    print(f\"\\nOVERALL PERFORMANCE:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Overall average - Llama-8B:  {overall_8b:.3f}\")\n",
    "    print(f\"Overall average - Llama-70B: {overall_70b:.3f}\")\n",
    "    print(f\"Overall improvement (70B vs 8B): {overall_70b - overall_8b:+.3f} ({((overall_70b/overall_8b - 1) * 100):+.1f}%)\")\n",
    "\n",
    "print_summary_comparison()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
