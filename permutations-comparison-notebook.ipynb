{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutations Task: Llama-8B vs Llama-70B Comparison\n",
    "\n",
    "This notebook compares the performance of different agent types on the permutations task between Llama-8B and Llama-70B models, plotting exact match and accuracy as a function of number of swaps with line plots and shaded error regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import scienceplots\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use(['science', 'ieee'])\n",
    "\n",
    "# LaTeX font settings for publication quality\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': True,\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 13,\n",
    "    'axes.labelsize': 12,\n",
    "    'legend.fontsize': 11,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "se_acc_df = pd.read_csv('data/se_acc_permutation_llama8B_70B.csv')\n",
    "se_em_df = pd.read_csv('data/se_em_permutation_llama8B_70B.csv')\n",
    "avg_em_df = pd.read_csv('data/avg_em_permutation_llama8B_70B.csv')\n",
    "avg_acc_df = pd.read_csv('data/avg_acc_permutation_llama8B_70B.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"SE Accuracy: {se_acc_df.shape}\")\n",
    "print(f\"SE Exact Match: {se_em_df.shape}\")\n",
    "print(f\"Avg Exact Match: {avg_em_df.shape}\")\n",
    "print(f\"Avg Accuracy: {avg_acc_df.shape}\")\n",
    "\n",
    "print(\"\\nExample columns from SE Accuracy:\")\n",
    "print([col for col in se_acc_df.columns if 'se_element_accuracy' in col][:3])\n",
    "print(\"\\nExample columns from Avg Exact Match:\")\n",
    "print([col for col in avg_em_df.columns if 'avg_exact_match' in col][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_method_and_model(col_name):\n",
    "    \"\"\"Extract method type and model size from column name\"\"\"\n",
    "    # Extract method\n",
    "    if 'prefix-sum' in col_name:\n",
    "        method = 'prefix-sum'\n",
    "    elif 'maj-voting' in col_name:\n",
    "        method = 'maj-voting'\n",
    "    elif 'coa' in col_name:\n",
    "        method = 'coa'\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "    # Extract model size\n",
    "    if 'llama8B' in col_name:\n",
    "        model = '8B'\n",
    "    elif 'llama70B' in col_name:\n",
    "        model = '70B'\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "    return method, model\n",
    "\n",
    "def extract_hyperparams(col_name):\n",
    "    \"\"\"Extract hyperparameters from column name\"\"\"\n",
    "    if 'maj-voting' in col_name:\n",
    "        match = re.search(r'agents(\\d+)', col_name)\n",
    "        return int(match.group(1)) if match else None\n",
    "    elif 'coa' in col_name:\n",
    "        match = re.search(r'chunk(\\d+)', col_name)\n",
    "        return int(match.group(1)) if match else None\n",
    "    elif 'prefix-sum' in col_name:\n",
    "        match = re.search(r'b(\\d+)', col_name)\n",
    "        return int(match.group(1)) if match else None\n",
    "    return None\n",
    "\n",
    "def process_data_for_plotting(df, metric_col_key, num_swaps_col='num_swaps'):\n",
    "    \"\"\"Process dataframe to extract best performance for each method-model combination\"\"\"\n",
    "    \n",
    "    # Handle different column names for number of swaps\n",
    "    if num_swaps_col not in df.columns:\n",
    "        if 'Step' in df.columns:\n",
    "            num_swaps_col = 'Step'\n",
    "        else:\n",
    "            print(f\"Warning: No swaps column found. Available columns: {df.columns.tolist()[:5]}...\")\n",
    "            return {}\n",
    "    \n",
    "    # Filter columns to remove MIN/MAX/step columns\n",
    "    filtered_df = df[[col for col in df.columns if all(x not in col for x in ['MIN', 'MAX', 'step']) and metric_col_key in col]]\n",
    "    \n",
    "    methods = ['prefix-sum', 'maj-voting', 'coa']\n",
    "    models = ['8B', '70B']\n",
    "    num_swaps = df[num_swaps_col]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        for model in models:\n",
    "            key = f'{method}_{model}'\n",
    "            results[key] = {'performance': [], 'hyperparams': [], 'std_err': []}\n",
    "            \n",
    "            # Find columns for this method-model combination\n",
    "            method_model_cols = [col for col in filtered_df.columns \n",
    "                               if method in col and f'llama{model}' in col]\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                # Find best performance for this method-model at this number of swaps\n",
    "                method_perfs = {}\n",
    "                for col in method_model_cols:\n",
    "                    perf = row[col]\n",
    "                    if pd.notna(perf) and perf != '':  # Only include non-NaN and non-empty values\n",
    "                        try:\n",
    "                            perf = float(perf)\n",
    "                            hyperparam = extract_hyperparams(col)\n",
    "                            if hyperparam is not None:\n",
    "                                method_perfs[hyperparam] = perf\n",
    "                        except (ValueError, TypeError):\n",
    "                            continue\n",
    "                \n",
    "                if method_perfs:\n",
    "                    best_hyperparam = max(method_perfs.keys(), key=lambda k: method_perfs[k])\n",
    "                    best_perf = method_perfs[best_hyperparam]\n",
    "                    results[key]['performance'].append(best_perf)\n",
    "                    results[key]['hyperparams'].append(best_hyperparam)\n",
    "                    # Calculate standard error assuming binomial distribution\n",
    "                    std_err = np.sqrt(best_perf * (1 - best_perf) / 100) if best_perf > 0 else 0\n",
    "                    results[key]['std_err'].append(std_err)\n",
    "                else:\n",
    "                    results[key]['performance'].append(np.nan)\n",
    "                    results[key]['hyperparams'].append(None)\n",
    "                    results[key]['std_err'].append(0)\n",
    "    \n",
    "    return results, num_swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data for plotting\n",
    "avg_em_results, num_swaps = process_data_for_plotting(avg_em_df, 'avg_exact_match')\n",
    "avg_acc_results, _ = process_data_for_plotting(avg_acc_df, 'avg_element_accuracy')\n",
    "\n",
    "print(\"Number of swaps:\", list(num_swaps))\n",
    "print(\"\\nAvailable method-model combinations:\")\n",
    "for key in avg_em_results.keys():\n",
    "    non_nan_count = sum(1 for x in avg_em_results[key]['performance'] if not np.isnan(x))\n",
    "    print(f\"{key}: {non_nan_count} valid data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_lines(em_results, acc_results, num_swaps):\n",
    "    \"\"\"Create line plots comparing 8B vs 70B for each method\"\"\"\n",
    "    \n",
    "    methods = ['prefix-sum', 'maj-voting', 'coa']\n",
    "    method_names = {'prefix-sum': 'Prefix Sum', 'maj-voting': 'Majority Voting', 'coa': 'Chain of Agents'}\n",
    "    \n",
    "    # Colors for 8B and 70B models\n",
    "    colors_8b = ['#4C72B0', '#55A868', '#C44E52']  # Blue, Green, Red for 8B\n",
    "    colors_70b = ['#1f4788', '#3d7c47', '#8b2635']  # Darker versions for 70B\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Convert to numpy array for easier indexing\n",
    "    x_vals = np.array(num_swaps)\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        # Exact Match plot (top row)\n",
    "        ax_em = axes[0, i]\n",
    "        \n",
    "        # Plot 8B model\n",
    "        key_8b = f'{method}_8B'\n",
    "        if key_8b in em_results:\n",
    "            y_vals_8b = np.array(em_results[key_8b]['performance'])\n",
    "            std_err_8b = np.array(em_results[key_8b]['std_err'])\n",
    "            \n",
    "            # Filter out NaN values\n",
    "            valid_mask = ~np.isnan(y_vals_8b)\n",
    "            if np.any(valid_mask):\n",
    "                x_valid = x_vals[valid_mask]\n",
    "                y_valid = y_vals_8b[valid_mask]\n",
    "                err_valid = std_err_8b[valid_mask]\n",
    "                \n",
    "                ax_em.plot(x_valid, y_valid, 'o-', color=colors_8b[i], \n",
    "                          linewidth=2, markersize=6, label='Llama-8B', alpha=0.8)\n",
    "                ax_em.fill_between(x_valid, \n",
    "                                  np.maximum(0, y_valid - err_valid),\n",
    "                                  np.minimum(1, y_valid + err_valid),\n",
    "                                  color=colors_8b[i], alpha=0.2)\n",
    "        \n",
    "        # Plot 70B model\n",
    "        key_70b = f'{method}_70B'\n",
    "        if key_70b in em_results:\n",
    "            y_vals_70b = np.array(em_results[key_70b]['performance'])\n",
    "            std_err_70b = np.array(em_results[key_70b]['std_err'])\n",
    "            \n",
    "            # Filter out NaN values\n",
    "            valid_mask = ~np.isnan(y_vals_70b)\n",
    "            if np.any(valid_mask):\n",
    "                x_valid = x_vals[valid_mask]\n",
    "                y_valid = y_vals_70b[valid_mask]\n",
    "                err_valid = std_err_70b[valid_mask]\n",
    "                \n",
    "                ax_em.plot(x_valid, y_valid, 's-', color=colors_70b[i], \n",
    "                          linewidth=2, markersize=6, label='Llama-70B', alpha=0.8)\n",
    "                ax_em.fill_between(x_valid, \n",
    "                                  np.maximum(0, y_valid - err_valid),\n",
    "                                  np.minimum(1, y_valid + err_valid),\n",
    "                                  color=colors_70b[i], alpha=0.2)\n",
    "        \n",
    "        ax_em.set_title(f'{method_names[method]} - Exact Match', fontweight='bold')\n",
    "        ax_em.set_xlabel('Number of Swaps')\n",
    "        ax_em.set_ylabel('Exact Match Accuracy')\n",
    "        ax_em.legend(frameon=False)\n",
    "        ax_em.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax_em.set_ylim(0, 1)\n",
    "        \n",
    "        # Element Accuracy plot (bottom row)\n",
    "        ax_acc = axes[1, i]\n",
    "        \n",
    "        # Plot 8B model\n",
    "        if key_8b in acc_results:\n",
    "            y_vals_8b = np.array(acc_results[key_8b]['performance'])\n",
    "            std_err_8b = np.array(acc_results[key_8b]['std_err'])\n",
    "            \n",
    "            # Filter out NaN values\n",
    "            valid_mask = ~np.isnan(y_vals_8b)\n",
    "            if np.any(valid_mask):\n",
    "                x_valid = x_vals[valid_mask]\n",
    "                y_valid = y_vals_8b[valid_mask]\n",
    "                err_valid = std_err_8b[valid_mask]\n",
    "                \n",
    "                ax_acc.plot(x_valid, y_valid, 'o-', color=colors_8b[i], \n",
    "                           linewidth=2, markersize=6, label='Llama-8B', alpha=0.8)\n",
    "                ax_acc.fill_between(x_valid, \n",
    "                                   np.maximum(0, y_valid - err_valid),\n",
    "                                   np.minimum(1, y_valid + err_valid),\n",
    "                                   color=colors_8b[i], alpha=0.2)\n",
    "        \n",
    "        # Plot 70B model\n",
    "        if key_70b in acc_results:\n",
    "            y_vals_70b = np.array(acc_results[key_70b]['performance'])\n",
    "            std_err_70b = np.array(acc_results[key_70b]['std_err'])\n",
    "            \n",
    "            # Filter out NaN values\n",
    "            valid_mask = ~np.isnan(y_vals_70b)\n",
    "            if np.any(valid_mask):\n",
    "                x_valid = x_vals[valid_mask]\n",
    "                y_valid = y_vals_70b[valid_mask]\n",
    "                err_valid = std_err_70b[valid_mask]\n",
    "                \n",
    "                ax_acc.plot(x_valid, y_valid, 's-', color=colors_70b[i], \n",
    "                           linewidth=2, markersize=6, label='Llama-70B', alpha=0.8)\n",
    "                ax_acc.fill_between(x_valid, \n",
    "                                   np.maximum(0, y_valid - err_valid),\n",
    "                                   np.minimum(1, y_valid + err_valid),\n",
    "                                   color=colors_70b[i], alpha=0.2)\n",
    "        \n",
    "        ax_acc.set_title(f'{method_names[method]} - Element Accuracy', fontweight='bold')\n",
    "        ax_acc.set_xlabel('Number of Swaps')\n",
    "        ax_acc.set_ylabel('Element Accuracy')\n",
    "        ax_acc.legend(frameon=False)\n",
    "        ax_acc.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax_acc.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('permutations_comparison_8B_vs_70B.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Create the comparison plots\n",
    "plot_comparison_lines(avg_em_results, avg_acc_results, num_swaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"Performance Summary by Method and Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "methods = ['prefix-sum', 'maj-voting', 'coa']\n",
    "models = ['8B', '70B']\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\n{method.upper().replace('-', ' ')}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model in models:\n",
    "        key = f'{method}_{model}'\n",
    "        \n",
    "        if key in avg_em_results:\n",
    "            em_perfs = [p for p in avg_em_results[key]['performance'] if not np.isnan(p)]\n",
    "            acc_perfs = [p for p in avg_acc_results[key]['performance'] if not np.isnan(p)]\n",
    "            \n",
    "            if em_perfs and acc_perfs:\n",
    "                print(f\"  Llama-{model}:\")\n",
    "                print(f\"    Exact Match  - Mean: {np.mean(em_perfs):.3f}, Max: {np.max(em_perfs):.3f}, Min: {np.min(em_perfs):.3f}\")\n",
    "                print(f\"    Element Acc  - Mean: {np.mean(acc_perfs):.3f}, Max: {np.max(acc_perfs):.3f}, Min: {np.min(acc_perfs):.3f}\")\n",
    "            else:\n",
    "                print(f\"  Llama-{model}: No valid data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified single plot showing overall comparison\n",
    "def plot_overall_comparison():\n",
    "    \"\"\"Create a simplified comparison plot showing best performance across methods\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Colors for each method\n",
    "    method_colors = {'prefix-sum': '#4C72B0', 'maj-voting': '#55A868', 'coa': '#C44E52'}\n",
    "    \n",
    "    x_vals = np.array(num_swaps)\n",
    "    \n",
    "    # Plot exact match comparison\n",
    "    for method in ['prefix-sum', 'maj-voting', 'coa']:\n",
    "        for model, linestyle, alpha in [('8B', '-', 0.7), ('70B', '--', 1.0)]:\n",
    "            key = f'{method}_{model}'\n",
    "            if key in avg_em_results:\n",
    "                y_vals = np.array(avg_em_results[key]['performance'])\n",
    "                valid_mask = ~np.isnan(y_vals)\n",
    "                \n",
    "                if np.any(valid_mask):\n",
    "                    x_valid = x_vals[valid_mask]\n",
    "                    y_valid = y_vals[valid_mask]\n",
    "                    \n",
    "                    label = f'{method.replace(\"-\", \" \").title()} ({model})'\n",
    "                    ax1.plot(x_valid, y_valid, linestyle=linestyle, \n",
    "                            color=method_colors[method], linewidth=2, \n",
    "                            alpha=alpha, label=label, marker='o' if model=='8B' else 's')\n",
    "    \n",
    "    ax1.set_title('Exact Match Accuracy Comparison', fontweight='bold')\n",
    "    ax1.set_xlabel('Number of Swaps')\n",
    "    ax1.set_ylabel('Exact Match Accuracy')\n",
    "    ax1.legend(frameon=False, fontsize=9)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot element accuracy comparison\n",
    "    for method in ['prefix-sum', 'maj-voting', 'coa']:\n",
    "        for model, linestyle, alpha in [('8B', '-', 0.7), ('70B', '--', 1.0)]:\n",
    "            key = f'{method}_{model}'\n",
    "            if key in avg_acc_results:\n",
    "                y_vals = np.array(avg_acc_results[key]['performance'])\n",
    "                valid_mask = ~np.isnan(y_vals)\n",
    "                \n",
    "                if np.any(valid_mask):\n",
    "                    x_valid = x_vals[valid_mask]\n",
    "                    y_valid = y_vals[valid_mask]\n",
    "                    \n",
    "                    label = f'{method.replace(\"-\", \" \").title()} ({model})'\n",
    "                    ax2.plot(x_valid, y_valid, linestyle=linestyle, \n",
    "                            color=method_colors[method], linewidth=2, \n",
    "                            alpha=alpha, label=label, marker='o' if model=='8B' else 's')\n",
    "    \n",
    "    ax2.set_title('Element Accuracy Comparison', fontweight='bold')\n",
    "    ax2.set_xlabel('Number of Swaps')\n",
    "    ax2.set_ylabel('Element Accuracy')\n",
    "    ax2.legend(frameon=False, fontsize=9)\n",
    "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('permutations_overall_comparison.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_overall_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots comparing all three agent types for each model separately\n",
    "def plot_agent_comparison_by_model():\n",
    "    \"\"\"Create 4 plots: EM and Accuracy for 8B, EM and Accuracy for 70B\"\"\"\n",
    "    \n",
    "    methods = ['prefix-sum', 'maj-voting', 'coa']\n",
    "    method_names = {'prefix-sum': 'Prefix Sum', 'maj-voting': 'Majority Voting', 'coa': 'Chain of Agents'}\n",
    "    \n",
    "    # Colors for the three methods\n",
    "    method_colors = ['#4C72B0', '#55A868', '#C44E52']  # Blue, Green, Red\n",
    "    \n",
    "    # Update font sizes to be bigger\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'axes.labelsize': 14,\n",
    "        'legend.fontsize': 12,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12\n",
    "    })\n",
    "    \n",
    "    x_vals = np.array(num_swaps)\n",
    "    \n",
    "    # Plot configurations for each model and metric combination\n",
    "    plot_configs = [\n",
    "        ('8B', avg_em_results, 'Exact Match'),\n",
    "        ('8B', avg_acc_results, 'Element Accuracy'), \n",
    "        ('70B', avg_em_results, 'Exact Match'),\n",
    "        ('70B', avg_acc_results, 'Element Accuracy')\n",
    "    ]\n",
    "    \n",
    "    # Create each plot separately\n",
    "    for model, results_dict, metric_name in plot_configs:\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        \n",
    "        for i, method in enumerate(methods):\n",
    "            key = f'{method}_{model}'\n",
    "            \n",
    "            if key in results_dict:\n",
    "                y_vals = np.array(results_dict[key]['performance'])\n",
    "                std_err = np.array(results_dict[key]['std_err'])\n",
    "                \n",
    "                # Filter out NaN values\n",
    "                valid_mask = ~np.isnan(y_vals)\n",
    "                \n",
    "                if np.any(valid_mask):\n",
    "                    x_valid = x_vals[valid_mask]\n",
    "                    y_valid = y_vals[valid_mask]\n",
    "                    err_valid = std_err[valid_mask]\n",
    "                    \n",
    "                    # Plot line with markers\n",
    "                    ax.plot(x_valid, y_valid, 'o-', color=method_colors[i], \n",
    "                           linewidth=2.5, markersize=6, label=method_names[method], \n",
    "                           alpha=0.8, markeredgecolor='white', markeredgewidth=0.5)\n",
    "                    \n",
    "                    # Add shaded error region\n",
    "                    ax.fill_between(x_valid, \n",
    "                                   np.maximum(0, y_valid - err_valid),\n",
    "                                   np.minimum(1, y_valid + err_valid),\n",
    "                                   color=method_colors[i], alpha=0.2)\n",
    "        \n",
    "        # Bold titles and axis labels\n",
    "        ax.set_title(rf'\\textbf{{Llama-{model}: {metric_name}}}')\n",
    "        ax.set_xlabel(rf'\\textbf{{Number of Swaps}}')\n",
    "        ax.set_ylabel(rf'\\textbf{{{metric_name}}}')\n",
    "        \n",
    "        # Use khop-notebook legend style\n",
    "        legend = ax.legend(frameon=True, loc='upper right', \n",
    "                          fancybox=True, shadow=True, framealpha=0.95,\n",
    "                          edgecolor='black', facecolor='white')\n",
    "        legend.get_frame().set_linewidth(0.8)\n",
    "        \n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        # Improve tick formatting\n",
    "        ax.tick_params(labelsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save each figure separately to figures directory\n",
    "        import os\n",
    "        os.makedirs('figures', exist_ok=True)\n",
    "        \n",
    "        # Create filename\n",
    "        metric_clean = metric_name.replace(' ', '_').lower()\n",
    "        filename = f'figures/permutations_{model.lower()}_{metric_clean}.pdf'\n",
    "        plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Saved: {filename}\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "# Create the agent comparison plots\n",
    "plot_agent_comparison_by_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
